{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor Classify","text":"<p>A fluent, type-safe API for text classification built on top of Instructor with support for multiple LLM providers.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83c\udff7\ufe0f Simple label definitions in YAML or Python</li> <li>\ud83d\udd12 Type-safe classification using Pydantic</li> <li>\ud83c\udf10 Multi-provider support (OpenAI, Anthropic, Google, etc.)</li> <li>\ud83d\udcca Complete evaluation framework with metrics and visualizations</li> <li>\u26a1 Async/Sync APIs for flexibility in different environments</li> <li>\ud83d\udcb0 Cost &amp; latency tracking for optimizing LLM usage</li> <li>\ud83e\uddea Statistical analysis with bootstrap confidence intervals</li> <li>\ud83d\ude80 CLI tools for project setup and evaluation</li> </ul>"},{"location":"#overview","title":"Overview","text":"<p>Instructor Classify provides a comprehensive framework for implementing, testing, and evaluating LLM-based text classification systems. It builds on the Instructor library to ensure type safety and structured outputs, while adding specific functionality for classification tasks.</p> <p>The package supports both single-label and multi-label classification, with flexible APIs for synchronous and asynchronous operation, batch processing, and detailed performance evaluation.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code># Install the package\npip install instructor-classify\n\n# Install your preferred LLM provider\npip install openai  # Or anthropic, google-generativeai, etc.\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":""},{"location":"#defining-classifications-in-python","title":"Defining Classifications in Python","text":"<p>Create classification schemas directly in code:</p> <pre><code>from instructor_classify import Classifier, ClassificationDefinition, LabelDefinition, Examples\nimport instructor\nfrom openai import OpenAI\n\n# Create label definitions\nquestion_label = LabelDefinition(\n    label=\"question\",\n    description=\"User is asking for information or clarification\",\n    examples=Examples(\n        examples_positive=[\n            \"What is machine learning?\",\n            \"How do I reset my password?\"\n        ],\n        examples_negative=[\n            \"Please book me a flight\",\n            \"I'd like to cancel my subscription\"\n        ]\n    )\n)\n\nrequest_label = LabelDefinition(\n    label=\"request\",\n    description=\"User wants the assistant to perform an action\",\n    examples=Examples(\n        examples_positive=[\n            \"Please book me a flight to Paris\",\n            \"Schedule a meeting for tomorrow at 2pm\"\n        ],\n        examples_negative=[\n            \"What is the capital of France?\",\n            \"Can you explain quantum physics?\"\n        ]\n    )\n)\n\n# Create classification definition\ndefinition = ClassificationDefinition(\n    label_definitions=[question_label, request_label]\n)\n\n# Create classifier\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(definition)\n    .with_client(client)\n    .with_model(\"gpt-3.5-turbo\")\n)\n\n# Make a prediction\nresult = classifier.predict(\"What is machine learning?\")\nprint(result.label)  # -&gt; \"question\"\n</code></pre>"},{"location":"#defining-classifications-in-yaml","title":"Defining Classifications in YAML","text":"<p>For better version control and reusability, you can define schemas in YAML:</p> <pre><code># prompt.yaml\nlabel_definitions:\n  - label: question\n    description: User is asking for information or clarification\n    examples:\n      examples_positive:\n        - \"What is machine learning?\"\n        - \"How do I reset my password?\"\n      examples_negative:\n        - \"Please book me a flight\"\n        - \"I'd like to cancel my subscription\"\n  - label: request\n    description: User wants the assistant to perform an action\n    examples:\n      examples_positive:\n        - \"Please book me a flight to Paris\"\n        - \"Schedule a meeting for tomorrow at 2pm\"\n      examples_negative:\n        - \"What is the capital of France?\"\n        - \"Can you explain quantum physics?\"\n</code></pre> <p>Then load it in your code:</p> <pre><code>from instructor_classify.classify import Classifier\nfrom instructor_classify.schema import ClassificationDefinition\nimport instructor\nfrom openai import OpenAI\n\n# Load classification definition from YAML\ndefinition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\n\n# Create classifier\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(definition)\n    .with_client(client)\n    .with_model(\"gpt-3.5-turbo\")\n)\n\n# Make a prediction\nresult = classifier.predict(\"What is machine learning?\")\nprint(result.label)  # -&gt; \"question\"\n</code></pre>"},{"location":"#using-the-cli","title":"Using the CLI","text":"<p>For quick project setup:</p> <pre><code># Set up your API key\nexport OPENAI_API_KEY=your-api-key-here\n\n# Initialize a new classification project\ninstruct-classify init my_classifier\ncd my_classifier\n</code></pre> <p>This creates a project with:</p> <ul> <li><code>prompt.yaml</code>: Sample classification schema definition</li> <li><code>example.py</code>: Sample code for using the classifier</li> <li><code>configs/</code>: Sample configurations</li> <li><code>datasets/</code>: Sample evaluation datasets</li> </ul> <p>This should be enough for you to get started testing models</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Getting Started</li> <li>Programmatic Definition</li> <li>Examples</li> <li>API Reference</li> <li>Evaluation Framework</li> <li>Contributing</li> </ul>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#cli-commands","title":"CLI Commands","text":""},{"location":"api/#instruct-classify-init","title":"<code>instruct-classify init</code>","text":"<p>Initialize a new classifier project.</p> <pre><code>instruct-classify init [PROJECT_NAME]\n</code></pre> <p>Creates a new project directory with: - <code>prompt.yaml</code>: Classification definition - <code>example.py</code>: Example code for using the classifier - <code>configs/</code>: Evaluation configurations - <code>datasets/</code>: Example evaluation datasets</p>"},{"location":"api/#instruct-classify-eval","title":"<code>instruct-classify eval</code>","text":"<p>Run evaluation using the unified evaluation framework.</p> <pre><code>instruct-classify eval --config CONFIG_PATH\n</code></pre> <p>Parameters: - <code>--config</code>, <code>-c</code>: Path to the evaluation configuration YAML file</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#classificationdefinition","title":"<code>ClassificationDefinition</code>","text":"<pre><code>from instructor_classify.schema import ClassificationDefinition\n</code></pre> <p>The schema for defining classification labels and examples.</p> <p>Methods:</p> <ul> <li><code>from_yaml(yaml_path: str) -&gt; ClassificationDefinition</code>: Load definition from YAML</li> <li><code>get_classification_model() -&gt; type[BaseModel]</code>: Get the single-label Pydantic model</li> <li><code>get_multiclassification_model() -&gt; type[BaseModel]</code>: Get the multi-label Pydantic model</li> </ul> <p>Example: <pre><code>definition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\n</code></pre></p>"},{"location":"api/#classifier","title":"<code>Classifier</code>","text":"<pre><code>from instructor_classify.classify import Classifier\n</code></pre> <p>Fluent API for single and multi-label classification with synchronous operations.</p> <p>Methods:</p> <ul> <li><code>with_client(client: instructor.Instructor) -&gt; Classifier</code>: Attach an LLM client</li> <li><code>with_model(model_name: str) -&gt; Classifier</code>: Specify the model to use</li> <li><code>predict(text: str) -&gt; BaseModel</code>: Make a single-label prediction</li> <li><code>predict_multi(text: str) -&gt; BaseModel</code>: Make a multi-label prediction</li> <li><code>batch_predict(texts: list[str], n_jobs: int | None = None) -&gt; list[BaseModel]</code>: Process multiple texts in parallel</li> <li><code>batch_predict_multi(texts: list[str], n_jobs: int | None = None) -&gt; list[BaseModel]</code>: Multi-label batch processing</li> <li><code>predict_with_completion(text: str) -&gt; tuple[BaseModel, Any]</code>: Return both parsed model and raw completion</li> <li><code>predict_multi_with_completion(text: str) -&gt; tuple[BaseModel, Any]</code>: Multi-label variant with raw completion</li> </ul> <p>Example: <pre><code>classifier = (\n    Classifier(definition)\n    .with_client(instructor.from_openai(OpenAI()))\n    .with_model(\"gpt-3.5-turbo\")\n)\nresult = classifier.predict(\"What is machine learning?\")\n</code></pre></p>"},{"location":"api/#asyncclassifier","title":"<code>AsyncClassifier</code>","text":"<pre><code>from instructor_classify.classify import AsyncClassifier\n</code></pre> <p>Asynchronous variant of the Classifier API. All prediction methods are coroutines.</p> <p>Methods:</p> <p>Same as Classifier, but all prediction methods are async and must be awaited.</p> <p>Example: <pre><code>classifier = (\n    AsyncClassifier(definition)\n    .with_client(instructor.from_openai_aclient(AsyncOpenAI()))\n    .with_model(\"gpt-4o\")\n)\nresult = await classifier.predict(\"What is machine learning?\")\nresults = await classifier.batch_predict(texts, n_jobs=10)\n</code></pre></p>"},{"location":"api/#evalset","title":"<code>EvalSet</code>","text":"<pre><code>from instructor_classify.schema import EvalSet\n</code></pre> <p>Holds a set of examples for evaluating classifier performance.</p> <p>Methods:</p> <ul> <li><code>from_yaml(yaml_path: str) -&gt; EvalSet</code>: Load from YAML file</li> <li><code>validate_against_definition(definition: ClassificationDefinition) -&gt; bool</code>: Validate labels match definition</li> </ul> <p>Example: <pre><code>eval_set = EvalSet.from_yaml(\"datasets/evalset_single.yaml\")\n</code></pre></p>"},{"location":"api/#unifiedevaluator","title":"<code>UnifiedEvaluator</code>","text":"<pre><code>from instructor_classify.eval_harness.unified_eval import UnifiedEvaluator\n</code></pre> <p>Comprehensive evaluation framework for testing model performance.</p> <p>Methods:</p> <ul> <li><code>__init__(config_path: str)</code>: Initialize with configuration file</li> <li><code>prepare()</code>: Prepare evaluation resources</li> <li><code>run()</code>: Execute evaluation and generate reports</li> </ul> <p>Example: <pre><code>evaluator = UnifiedEvaluator(\"configs/eval_config.yaml\")\nevaluator.prepare()\nevaluator.run()\n</code></pre></p>"},{"location":"api/#yaml-configuration-schemas","title":"YAML Configuration Schemas","text":""},{"location":"api/#classification-definition-promptyaml","title":"Classification Definition (<code>prompt.yaml</code>)","text":"<pre><code>system_message: |\n  You are an expert classification system...\n\nlabel_definitions:\n  - label: string\n    description: string\n    examples:\n      examples_positive:\n        - string\n        - string\n      examples_negative:\n        - string\n        - string\n</code></pre>"},{"location":"api/#evaluation-set-evalsetyaml","title":"Evaluation Set (<code>evalset.yaml</code>)","text":"<pre><code>name: \"Evaluation Set Name\"\ndescription: \"Description of the evaluation set\"\nclassification_type: \"single\" # or \"multi\"\nexamples:\n  - text: \"Example text to classify\"\n    expected_label: \"expected_label\" # for single-label\n  - text: \"Another example\"\n    expected_labels: [\"label1\", \"label2\"] # for multi-label\n</code></pre>"},{"location":"api/#evaluation-configuration-configyaml","title":"Evaluation Configuration (<code>config.yaml</code>)","text":"<pre><code># Models to evaluate\nmodels:\n  - \"gpt-3.5-turbo\"\n  - \"gpt-4o-mini\"\n\n# Evaluation datasets\neval_sets:\n  - \"datasets/evalset_multi.yaml\"\n  - \"datasets/evalset_single.yaml\"\n\n# Analysis parameters\nbootstrap_samples: 1000\nconfidence_level: 0.95\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The following exceptions may be raised:</p> <ul> <li><code>FileNotFoundError</code>: When the specified files cannot be found</li> <li><code>yaml.YAMLError</code>: When YAML parsing fails</li> <li><code>ValueError</code>: For invalid configurations or parameters</li> <li>Provider-specific errors: When LLM API calls fail</li> </ul>"},{"location":"contributing/","title":"Contributing to Instructor Classify","text":"<p>We welcome contributions to Instructor Classify! This document provides guidelines and instructions for contributing.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Fork the repository and clone your fork:    <pre><code>git clone https://github.com/your-username/instructor-classify.git\ncd instructor-classify\n</code></pre></p> </li> <li> <p>Create a virtual environment (optional but recommended):    <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install in development mode with dev dependencies:    <pre><code>pip install -e \".[dev]\"\n</code></pre></p> </li> <li> <p>Install pre-commit hooks:    <pre><code>pre-commit install\n</code></pre></p> </li> </ol>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Create a new branch for your feature or bugfix:    <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes, following the code style guidelines</p> </li> <li> <p>Run tests to ensure your changes don't break existing functionality:    <pre><code>pytest\n</code></pre></p> </li> <li> <p>Update documentation if needed:    <pre><code>mkdocs serve  # To preview documentation locally\n</code></pre></p> </li> <li> <p>Commit your changes with a descriptive commit message:    <pre><code>git add .\ngit commit -m \"feat: Add new classification feature\"\n</code></pre></p> </li> <li> <p>Push your branch to your fork:    <pre><code>git push origin feature/your-feature-name\n</code></pre></p> </li> <li> <p>Create a pull request on GitHub</p> </li> </ol>"},{"location":"contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>We use conventional commits for our commit messages:</p> <ul> <li><code>feat:</code> for new features</li> <li><code>fix:</code> for bug fixes</li> <li><code>docs:</code> for documentation changes</li> <li><code>style:</code> for formatting changes</li> <li><code>refactor:</code> for code restructuring without changing functionality</li> <li><code>test:</code> for adding or modifying tests</li> <li><code>chore:</code> for routine tasks, dependency updates, etc.</li> </ul> <p>Example: <code>feat: Add support for Claude models</code></p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>All new features and bug fixes should include tests. We use pytest for testing:</p> <pre><code># Run all tests\npytest\n\n# Run specific tests\npytest tests/test_classifier.py\n\n# Run tests with coverage\npytest --cov=instructor_classify\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Documentation is written in Markdown and built with MkDocs:</p> <ul> <li>All new features should be documented</li> <li>Update existing documentation as needed</li> <li>Include code examples to demonstrate usage</li> <li>Run <code>mkdocs serve</code> to preview documentation locally before submitting</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure your code follows the style guidelines</li> <li>Ensure all tests pass</li> <li>Update documentation as needed</li> <li>Submit your pull request with a clear title and description</li> <li>Respond to any feedback or requested changes</li> </ol>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>For maintainers:</p> <ol> <li>Update version in <code>pyproject.toml</code> following Semantic Versioning</li> <li>Update <code>CHANGELOG.md</code> with all notable changes</li> <li>Create a new release on GitHub with release notes</li> <li>The CI workflow will automatically publish to PyPI</li> </ol>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":"<p>When adding new features, consider:</p> <ol> <li>Backward Compatibility: Ensure existing code continues to work</li> <li>Performance: New features should not significantly degrade performance</li> <li>Testing: Include comprehensive tests</li> <li>Documentation: Add clear documentation with examples</li> </ol>"},{"location":"contributing/#questions-and-support","title":"Questions and Support","text":"<p>If you have questions about contributing:</p> <ul> <li>Open an issue on GitHub</li> <li>Ask in the repository's discussion section</li> <li>Contact the maintainers directly</li> </ul> <p>Thank you for contributing to Instructor Classify!</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>pip (Python package installer)</li> </ul>"},{"location":"installation/#installation-options","title":"Installation Options","text":""},{"location":"installation/#from-pypi","title":"From PyPI","text":"<pre><code>pip install instructor-classify\n</code></pre>"},{"location":"installation/#from-source","title":"From Source","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/jasonliu/instructor-classify.git\ncd instructor-classify\n</code></pre></p> </li> <li> <p>Install in development mode: <pre><code>pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"installation/#required-dependencies","title":"Required Dependencies","text":"<p>The package automatically installs these core dependencies: - <code>instructor&gt;=0.3.0</code>: Foundation for LLM structured output - <code>pydantic&gt;=2.0.0</code>: Type validation and schema generation - <code>typer&gt;=0.9.0</code>: CLI interface - <code>pyyaml&gt;=6.0.0</code>: Configuration handling - <code>numpy&gt;=2.0.2</code>, <code>matplotlib&gt;=3.9.4</code>, <code>seaborn&gt;=0.13.2</code>: Visualization support - <code>scikit-learn&gt;=1.6.1</code>: Evaluation metrics and analysis</p>"},{"location":"installation/#llm-provider-libraries","title":"LLM Provider Libraries","text":"<p>You'll need to install at least one LLM provider client:</p> <pre><code># For OpenAI\npip install openai\n\n# For Anthropic\npip install anthropic\n\n# For Google\npip install google-generativeai\n\n# For other providers supported by Instructor\n# See the Instructor documentation\n</code></pre>"},{"location":"installation/#api-keys","title":"API Keys","text":"<p>Set up your API key for your chosen LLM provider:</p> <pre><code># For OpenAI\nexport OPENAI_API_KEY=your-api-key-here\n\n# For Anthropic\nexport ANTHROPIC_API_KEY=your-api-key-here\n\n# For Google\nexport GOOGLE_API_KEY=your-api-key-here\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For the best experience, you may want to install:</p> <pre><code># Progress bars\npip install tqdm\n\n# Documentation\npip install mkdocs mkdocs-material\n</code></pre>"},{"location":"installation/#verifying-installation","title":"Verifying Installation","text":"<p>To verify that the installation was successful, run:</p> <pre><code>instruct-classify --help\n</code></pre> <p>You should see the help message with available commands.</p>"},{"location":"usage/evaluation/","title":"Evaluation Framework","text":"<p>Instructor Classify includes a comprehensive evaluation framework for testing and comparing model performance on classification tasks.</p>"},{"location":"usage/evaluation/#overview","title":"Overview","text":"<p>The evaluation framework provides:</p> <ul> <li>Performance metrics (accuracy, precision, recall, F1 score)</li> <li>Statistical analysis with bootstrap confidence intervals</li> <li>Error analysis with confusion matrices</li> <li>Cost and latency tracking</li> <li>Visualizations and detailed reports</li> </ul>"},{"location":"usage/evaluation/#running-evaluations","title":"Running Evaluations","text":"<p>You can run evaluations using the CLI:</p> <pre><code>instruct-classify eval --config configs/example.yaml\n</code></pre>"},{"location":"usage/evaluation/#configuration","title":"Configuration","text":"<p>The evaluation configuration is defined in a YAML file:</p> <pre><code># Models to evaluate, we will search all models\nmodels:\n  - \"gpt-3.5-turbo\"\n  - \"gpt-4o-mini\"\n\n# Evaluation datasets\n# We can segment each one based on certain splits \neval_sets:\n  - \"datasets/evalset_multi.yaml\"\n  - \"datasets/evalset_single.yaml\"\n\n# Analysis parameters\nbootstrap_samples: 1000\nconfidence_level: 0.95\n\n# Optional parameters\noutput_dir: \"results\"  # Where to save results\nverbose: true          # Show detailed progress\n</code></pre>"},{"location":"usage/evaluation/#evaluation-datasets","title":"Evaluation Datasets","text":"<p>Evaluation datasets are defined in YAML files:</p> <pre><code>name: \"Custom Classification Evaluation Set\"\ndescription: \"A set of examples for testing intent classification\"\nclassification_type: \"single\"  # or \"multi\"\nexamples:\n  - text: \"How do I reset my password?\"\n    expected_label: \"account_question\"\n  - text: \"I need to update my billing information.\"\n    expected_label: \"billing_request\"\n  - text: \"What time do you close today?\"\n    expected_label: \"general_question\"\n  # Add more examples...\n</code></pre> <p>For multi-label classification, use <code>expected_labels</code> instead:</p> <pre><code>  - text: \"I'm having trouble logging in and need to update my payment method.\"\n    expected_labels: [\"account_question\", \"billing_request\"]\n</code></pre>"},{"location":"usage/evaluation/#outputs","title":"Outputs","text":"<p>The evaluation framework generates a comprehensive set of outputs:</p>"},{"location":"usage/evaluation/#1-summary-report","title":"1. Summary Report","text":"<p>A text file with overall results:</p> <pre><code># Evaluation Summary Report\n\nDate: 2025-04-18 19:31:16\n\n## Models Evaluated\n- gpt-3.5-turbo\n- gpt-4o-mini\n\n## Datasets\n- Complex Classification Evaluation Set\n- Custom Classification Evaluation Set\n\n## Overall Performance\n              | gpt-3.5-turbo | gpt-4o-mini  |\n--------------|--------------|--------------|\nAccuracy      | 0.8500       | 0.9250       |\nMacro F1      | 0.8479       | 0.9268       |\nAvg. Latency  | 0.6521s      | 0.8752s      |\nCost (tokens) | 21,450       | 24,680       |\n\n## Bootstrap Confidence Intervals (95%)\n              | gpt-3.5-turbo     | gpt-4o-mini       |\n--------------|------------------|-------------------|\nAccuracy      | 0.8025 - 0.8975  | 0.8850 - 0.9650   |\n</code></pre>"},{"location":"usage/evaluation/#2-metrics-files","title":"2. Metrics Files","text":"<p>Detailed JSON files for each model and dataset:</p> <pre><code>{\n  \"accuracy\": 0.925,\n  \"macro_precision\": 0.9325,\n  \"macro_recall\": 0.9231,\n  \"macro_f1\": 0.9268,\n  \"per_label_metrics\": {\n    \"account_question\": {\n      \"precision\": 0.9545,\n      \"recall\": 0.9545,\n      \"f1\": 0.9545\n    },\n    \"billing_request\": {\n      \"precision\": 0.9333,\n      \"recall\": 0.9333,\n      \"f1\": 0.9333\n    },\n    \"general_question\": {\n      \"precision\": 0.9091,\n      \"recall\": 0.8824,\n      \"f1\": 0.8955\n    }\n  }\n}\n</code></pre>"},{"location":"usage/evaluation/#3-visualizations","title":"3. Visualizations","text":"<p>The framework generates various visualizations:</p> <ul> <li>Confusion matrices for each model/dataset</li> <li>Error distribution charts</li> <li>Bootstrap confidence interval visualizations</li> <li>Cost and latency comparisons</li> </ul>"},{"location":"usage/evaluation/#4-cost-and-latency-analysis","title":"4. Cost and Latency Analysis","text":"<p>JSON files with detailed cost and latency data:</p> <pre><code>{\n  \"models\": {\n    \"gpt-3.5-turbo\": {\n      \"total_tokens\": 21450,\n      \"estimated_cost_usd\": 0.0429,\n      \"avg_tokens_per_prediction\": 214.5,\n      \"avg_latency_seconds\": 0.6521\n    },\n    \"gpt-4o-mini\": {\n      \"total_tokens\": 24680,\n      \"estimated_cost_usd\": 0.0494,\n      \"avg_tokens_per_prediction\": 246.8,\n      \"avg_latency_seconds\": 0.8752\n    }\n  }\n}\n</code></pre>"},{"location":"usage/evaluation/#advanced-analysis","title":"Advanced Analysis","text":""},{"location":"usage/evaluation/#bootstrap-analysis","title":"Bootstrap Analysis","text":"<p>The evaluation framework uses bootstrap resampling to estimate confidence intervals:</p> <pre><code>{\n  \"bootstrap_samples\": 1000,\n  \"confidence_level\": 0.95,\n  \"metrics\": {\n    \"gpt-3.5-turbo\": {\n      \"Complex Classification Evaluation Set\": {\n        \"accuracy\": {\n          \"mean\": 0.85,\n          \"lower_bound\": 0.8025,\n          \"upper_bound\": 0.8975\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"usage/evaluation/#confusion-matrix-analysis","title":"Confusion Matrix Analysis","text":"<p>Detailed confusion matrices help identify specific error patterns:</p> <pre><code>{\n  \"gpt-3.5-turbo\": {\n    \"Custom Classification Evaluation Set\": {\n      \"account_question\": {\n        \"account_question\": 21,\n        \"billing_request\": 1,\n        \"general_question\": 0\n      },\n      \"billing_request\": {\n        \"account_question\": 1,\n        \"billing_request\": 14,\n        \"general_question\": 0\n      },\n      \"general_question\": {\n        \"account_question\": 0,\n        \"billing_request\": 1,\n        \"general_question\": 16\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"usage/evaluation/#programmatic-access-to-results","title":"Programmatic Access to Results","text":"<p>You can access evaluation results programmatically:</p> <pre><code>from instructor_classify.eval_harness.unified_eval import UnifiedEvaluator\nimport json\n\n# Run evaluation\nevaluator = UnifiedEvaluator(\"configs/example.yaml\")\nevaluator.prepare()\nresults = evaluator.run()\n\n# Access results\nfor model_name, model_results in results.items():\n    print(f\"Model: {model_name}\")\n    for eval_set_name, metrics in model_results.items():\n        print(f\"  Dataset: {eval_set_name}\")\n        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n\n        # Check if this model is better than others with statistical significance\n        bootstrap_data = evaluator.bootstrap_results[\"metrics\"][model_name][eval_set_name][\"accuracy\"]\n        print(f\"  95% CI: [{bootstrap_data['lower_bound']:.4f}, {bootstrap_data['upper_bound']:.4f}]\")\n</code></pre>"},{"location":"usage/evaluation/#custom-evaluation-metrics","title":"Custom Evaluation Metrics","text":"<p>You can extend the evaluation framework with custom metrics:</p> <pre><code>from instructor_classify.eval_harness.unified_eval import UnifiedEvaluator\nfrom sklearn.metrics import matthews_corrcoef\n\nclass CustomEvaluator(UnifiedEvaluator):\n    def calculate_metrics(self, true_labels, pred_labels):\n        # Get standard metrics\n        metrics = super().calculate_metrics(true_labels, pred_labels)\n\n        # Add Matthews Correlation Coefficient\n        mcc = matthews_corrcoef(true_labels, pred_labels)\n        metrics[\"matthews_corrcoef\"] = mcc\n\n        return metrics\n\n# Use custom evaluator\nevaluator = CustomEvaluator(\"configs/example.yaml\")\nevaluator.prepare()\nresults = evaluator.run()\n</code></pre>"},{"location":"usage/examples/","title":"Examples","text":"<p>This page provides examples of common classification tasks and advanced usage patterns.</p>"},{"location":"usage/examples/#intent-classification","title":"Intent Classification","text":"<pre><code>from instructor_classify import Classifier, ClassificationDefinition\nimport instructor\nfrom openai import OpenAI\n\n# Intent classification definition\nyaml_content = \"\"\"\nsystem_message: |\n  You are an expert intent classifier for a customer service system.\n\nlabel_definitions:\n  - label: account_issue\n    description: The user has a problem with their account access or settings.\n    examples:\n      examples_positive:\n        - \"I can't log into my account\"\n        - \"How do I change my password?\"\n      examples_negative:\n        - \"When will my order arrive?\"\n        - \"I want to return this product\"\n\n  - label: billing_question\n    description: The user has a question about billing, charges, or payments.\n    examples:\n      examples_positive:\n        - \"Why was I charged twice?\"\n        - \"When will my subscription renew?\"\n      examples_negative:\n        - \"How do I track my order?\"\n        - \"The product is broken\"\n\n  - label: product_support\n    description: The user needs help with using a product or has a technical issue.\n    examples:\n      examples_positive:\n        - \"My device won't turn on\"\n        - \"How do I reset the software?\"\n      examples_negative:\n        - \"I want to cancel my order\"\n        - \"Can I change my shipping address?\"\n\"\"\"\n\n# Save to file\nwith open(\"intent_classifier.yaml\", \"w\") as f:\n    f.write(yaml_content)\n\n# Load and use\ndefinition = ClassificationDefinition.from_yaml(\"intent_classifier.yaml\")\nclient = instructor.from_openai(OpenAI())\nclassifier = Classifier(definition).with_client(client).with_model(\"gpt-3.5-turbo\")\n\n# Test with example queries\nqueries = [\n    \"I forgot my password and now I can't get into my account\",\n    \"You charged me twice for my last order\",\n    \"The app keeps crashing whenever I try to upload photos\",\n    \"When will my order arrive?\",\n]\n\nfor query in queries:\n    result = classifier.predict(query)\n    print(f\"Query: '{query}'\")\n    print(f\"Intent: {result.label}\")\n    print(\"---\")\n</code></pre>"},{"location":"usage/examples/#multi-label-classification","title":"Multi-Label Classification","text":"<pre><code>from instructor_classify import Classifier, ClassificationDefinition\nimport instructor\nfrom openai import OpenAI\nimport yaml\n\n# Content classifier for multiple labels\nyaml_content = \"\"\"\nsystem_message: |\n  You are an expert content classifier for a content moderation system.\n  Content may belong to multiple categories simultaneously.\n\nlabel_definitions:\n  - label: politics\n    description: Content related to government, policies, elections, or political figures.\n\n  - label: business\n    description: Content related to companies, markets, finance, or the economy.\n\n  - label: technology\n    description: Content related to digital products, software, hardware, or scientific innovation.\n\n  - label: entertainment\n    description: Content related to movies, music, celebrities, or leisure activities.\n\n  - label: sports\n    description: Content related to athletics, games, competitions, or sporting events.\n\"\"\"\n\nwith open(\"content_classifier.yaml\", \"w\") as f:\n    f.write(yaml_content)\n\n# Load and use\ndefinition = ClassificationDefinition.from_yaml(\"content_classifier.yaml\")\nclient = instructor.from_openai(OpenAI())\nclassifier = Classifier(definition).with_client(client).with_model(\"gpt-4o-mini\")\n\n# Example articles to classify\narticles = [\n    \"Apple announces record profits as new iPhone sales exceed expectations.\",\n    \"Senate passes new tech regulation bill aimed at social media companies.\",\n    \"Hollywood actors strike over streaming revenue and AI concerns.\",\n    \"Tech giant releases new AI tools for small business accounting.\",\n]\n\nfor article in articles:\n    result = classifier.predict_multi(article)\n    print(f\"Article: '{article}'\")\n    print(f\"Categories: {result.labels}\")\n    print(\"---\")\n</code></pre>"},{"location":"usage/examples/#working-with-raw-completions","title":"Working with Raw Completions","text":"<pre><code>from instructor_classify.classify import Classifier\nfrom instructor_classify.schema import ClassificationDefinition\nimport instructor\nfrom openai import OpenAI\n\n# Load your classification definition\ndefinition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\nclient = instructor.from_openai(OpenAI())\nclassifier = Classifier(definition).with_client(client).with_model(\"gpt-3.5-turbo\")\n\n# Get both the structured output and the raw completion\ntext = \"What is the capital of France?\"\nresult, completion = classifier.predict_with_completion(text)\n\nprint(f\"Structured result: {result.label}\")\nprint(f\"Confidence: {completion.choices[0].finish_reason}\")\nprint(f\"Model: {completion.model}\")\nprint(f\"Usage: {completion.usage.total_tokens} tokens\")\n\n# You can analyze the completion for additional insights\nresponse_text = completion.choices[0].message.content\nprint(f\"Raw response: {response_text}\")\n</code></pre>"},{"location":"usage/examples/#asynchronous-batch-processing","title":"Asynchronous Batch Processing","text":"<pre><code>from instructor_classify.classify import AsyncClassifier\nfrom instructor_classify.schema import ClassificationDefinition\nimport instructor\nfrom openai import AsyncOpenAI\nimport asyncio\nimport time\n\nasync def classify_large_dataset():\n    # Load your classification definition\n    definition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\n    client = instructor.from_openai_aclient(AsyncOpenAI())\n    classifier = AsyncClassifier(definition).with_client(client).with_model(\"gpt-3.5-turbo\")\n\n    # Sample large dataset\n    dataset = [\n        \"How do I reset my password?\",\n        \"I'd like to cancel my subscription\",\n        \"What are your business hours?\",\n        \"The product I received is damaged\",\n        \"Do you ship internationally?\",\n        # ... imagine hundreds more items\n    ]\n\n    # Process in batches with concurrency control\n    start_time = time.time()\n    results = await classifier.batch_predict(dataset, n_jobs=10)\n    end_time = time.time()\n\n    # Analyze results\n    label_counts = {}\n    for result in results:\n        label_counts[result.label] = label_counts.get(result.label, 0) + 1\n\n    print(f\"Processed {len(dataset)} items in {end_time - start_time:.2f} seconds\")\n    print(f\"Distribution: {label_counts}\")\n\n    return results\n\n# Run the async function\nif __name__ == \"__main__\":\n    asyncio.run(classify_large_dataset())\n</code></pre>"},{"location":"usage/examples/#using-different-llm-providers","title":"Using Different LLM Providers","text":"<pre><code>from instructor_classify.classify import Classifier\nfrom instructor_classify.schema import ClassificationDefinition\nimport instructor\n\n# Define a function to get results from different providers\ndef compare_providers(text, definition):\n    results = {}\n\n    # OpenAI\n    try:\n        from openai import OpenAI\n        openai_client = instructor.from_openai(OpenAI())\n        openai_classifier = (\n            Classifier(definition)\n            .with_client(openai_client)\n            .with_model(\"gpt-3.5-turbo\")\n        )\n        results[\"openai\"] = openai_classifier.predict(text).label\n    except Exception as e:\n        results[\"openai\"] = f\"Error: {str(e)}\"\n\n    # Anthropic\n    try:\n        from anthropic import Anthropic\n        anthropic_client = instructor.from_anthropic(Anthropic())\n        anthropic_classifier = (\n            Classifier(definition)\n            .with_client(anthropic_client)\n            .with_model(\"claude-3-haiku-20240307\")\n        )\n        results[\"anthropic\"] = anthropic_classifier.predict(text).label\n    except Exception as e:\n        results[\"anthropic\"] = f\"Error: {str(e)}\"\n\n    # Google\n    try:\n        import google.generativeai as genai\n        google_client = instructor.from_gemini(genai)\n        google_classifier = (\n            Classifier(definition)\n            .with_client(google_client)\n            .with_model(\"gemini-pro\")\n        )\n        results[\"google\"] = google_classifier.predict(text).label\n    except Exception as e:\n        results[\"google\"] = f\"Error: {str(e)}\"\n\n    return results\n\n# Example usage\ndefinition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\ntext = \"What is the best way to learn programming?\"\n\nresults = compare_providers(text, definition)\nfor provider, label in results.items():\n    print(f\"{provider}: {label}\")\n</code></pre>"},{"location":"usage/examples/#integration-with-web-frameworks","title":"Integration with Web Frameworks","text":""},{"location":"usage/examples/#fastapi-example","title":"FastAPI Example","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom instructor_classify.classify import Classifier\nfrom instructor_classify.schema import ClassificationDefinition\nimport instructor\nfrom openai import OpenAI\nimport os\n\napp = FastAPI(title=\"Text Classification API\")\n\n# Load classifier on startup\n@app.on_event(\"startup\")\nasync def startup_event():\n    global classifier\n\n    # Load classification definition\n    definition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\n\n    # Create classifier\n    client = instructor.from_openai(OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")))\n    classifier = (\n        Classifier(definition)\n        .with_client(client)\n        .with_model(\"gpt-3.5-turbo\")\n    )\n\nclass ClassificationRequest(BaseModel):\n    text: str\n\nclass ClassificationResponse(BaseModel):\n    text: str\n    label: str\n\n@app.post(\"/classify\", response_model=ClassificationResponse)\nasync def classify_text(request: ClassificationRequest):\n    try:\n        result = classifier.predict(request.text)\n        return ClassificationResponse(\n            text=request.text,\n            label=result.label\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Run with: uvicorn app:app --reload\n</code></pre>"},{"location":"usage/examples/#running-evaluations-from-code","title":"Running Evaluations From Code","text":"<pre><code>from instructor_classify.eval_harness.unified_eval import UnifiedEvaluator\nimport yaml\n\n# Create configuration programmatically\neval_config = {\n    \"models\": [\"gpt-3.5-turbo\", \"gpt-4o-mini\"],\n    \"eval_sets\": [\"datasets/evalset_single.yaml\"],\n    \"bootstrap_samples\": 1000,\n    \"confidence_level\": 0.95\n}\n\n# Save to file\nwith open(\"dynamic_eval_config.yaml\", \"w\") as f:\n    yaml.dump(eval_config, f)\n\n# Run evaluation\nevaluator = UnifiedEvaluator(\"dynamic_eval_config.yaml\")\nevaluator.prepare()\nresults = evaluator.run()\n\n# Access evaluation results programmatically\nfor model_name, model_results in results.items():\n    print(f\"Model: {model_name}\")\n    for eval_set_name, metrics in model_results.items():\n        print(f\"  Eval Set: {eval_set_name}\")\n        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"  F1 Score: {metrics['macro_f1']:.4f}\")\n</code></pre>"},{"location":"usage/getting-started/","title":"Getting Started","text":"<p>This guide will help you set up and use Instructor Classify for LLM-based text classification.</p>"},{"location":"usage/getting-started/#project-initialization","title":"Project Initialization","text":"<p>Create a new classification project:</p> <pre><code>instruct-classify init my_classifier\ncd my_classifier\n</code></pre> <p>This creates a project with: - <code>prompt.yaml</code>: Classification schema definition - <code>example.py</code>: Example code for using the classifier - <code>configs/</code>: Evaluation configurations - <code>datasets/</code>: Example evaluation datasets</p>"},{"location":"usage/getting-started/#understanding-the-key-files","title":"Understanding the Key Files","text":""},{"location":"usage/getting-started/#promptyaml","title":"prompt.yaml","text":"<p>The <code>prompt.yaml</code> file defines your classification schema, which is used to classify the user's input. Its often recommended to use an LLM to generate this schema.</p> <pre><code>system_message: |\n  You are an expert classification system designed to analyse user inputs.\n\nlabel_definitions:\n  - label: question\n    description: The user asks for information or clarification.\n    examples:\n      examples_positive:\n        - \"What is the capital of France?\"\n        - \"How does this feature work?\"\n      examples_negative:\n        - \"Please book me a flight to Paris.\"\n        - \"I want to return this product.\"\n\n  - label: request\n    description: The user wants the assistant to perform an action.\n    examples:\n      examples_positive:\n        - \"Please book me a flight to Paris.\"\n        - \"Update my account settings.\"\n      examples_negative:\n        - \"What is the capital of France?\"\n        - \"I'm having a problem with my order.\"\n</code></pre> <p>Each label definition includes: - <code>label</code>: The category name (automatically converted to lowercase) - <code>description</code>: What this category represents - <code>examples</code>: Optional positive and negative examples to guide the model</p> <p>These are all prompts to the LLM</p> <p>The LLM will use these labels to classify the user's input. Changing the labels will change the behavior of the LLM.</p>"},{"location":"usage/getting-started/#examplepy","title":"example.py","text":"<p>The <code>example.py</code> file shows basic usage:</p> <pre><code>from instructor_classify.classify import Classifier\nfrom instructor_classify.schema import ClassificationDefinition\nimport instructor\nfrom openai import OpenAI\n\n# Load classification definition\ndefinition = ClassificationDefinition.from_yaml(\"prompt.yaml\")\n\n# Create classifier\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(definition)\n    .with_client(client)\n    .with_model(\"gpt-3.5-turbo\")\n)\n\n# Make predictions\nresult = classifier.predict(\"What is machine learning?\")\nprint(result.label)  # -&gt; \"question\"\n</code></pre>"},{"location":"usage/getting-started/#basic-classification","title":"Basic Classification","text":""},{"location":"usage/getting-started/#single-label-classification","title":"Single-Label Classification","text":"<pre><code># Single-label prediction\nresult = classifier.predict(\"What is the weather today?\")\nprint(f\"Label: {result.label}\") # -&gt; \"question\"\n</code></pre>"},{"location":"usage/getting-started/#multi-label-classification","title":"Multi-Label Classification","text":"<pre><code># Multi-label prediction\nresult = classifier.predict_multi(\"Can you help me find flights to Paris and book a hotel?\")\nprint(f\"Labels: {result.labels}\") # -&gt; [\"request\", \"question\"]\n</code></pre>"},{"location":"usage/getting-started/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple texts in parallel\ntexts = [\n    \"What is machine learning?\",\n    \"Please book a flight to New York.\",\n    \"Can you explain how to use this API?\"\n]\n\n# Synchronous batch processing\nresults = classifier.batch_predict(texts)\nfor text, result in zip(texts, results):\n    print(f\"Text: '{text}' \u2192 Label: {result.label}\")\n</code></pre>"},{"location":"usage/getting-started/#asynchronous-api","title":"Asynchronous API","text":"<p>For high-throughput applications, use the <code>AsyncClassifier</code>:</p> <pre><code>from instructor_classify.classify import AsyncClassifier\nfrom openai import AsyncOpenAI\nimport asyncio\n\nasync def main():\n    # Create async classifier\n    client = instructor.from_openai_aclient(AsyncOpenAI())\n    classifier = (\n        AsyncClassifier(definition)\n        .with_client(client)\n        .with_model(\"gpt-4o\")\n    )\n\n    # Make predictions\n    result = await classifier.predict(\"What is machine learning?\")\n    print(result.label)\n\n    # Batch processing with concurrency control\n    results = await classifier.batch_predict(texts, n_jobs=10)\n    for text, result in zip(texts, results):\n        print(f\"Text: '{text}' \u2192 Label: {result.label}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"usage/getting-started/#working-with-multiple-llm-providers","title":"Working with Multiple LLM Providers","text":"<p>Instructor Classify works with any provider supported by Instructor:</p>"},{"location":"usage/getting-started/#openai","title":"OpenAI","text":"<pre><code>from openai import OpenAI\nclient = instructor.from_openai(OpenAI())\n</code></pre>"},{"location":"usage/getting-started/#anthropic","title":"Anthropic","text":"<pre><code>from anthropic import Anthropic\nclient = instructor.from_anthropic(Anthropic())\n</code></pre>"},{"location":"usage/getting-started/#google","title":"Google","text":"<pre><code>import google.generativeai as genai\nclient = instructor.from_gemini(genai)\n</code></pre>"},{"location":"usage/getting-started/#customizing-your-classifier","title":"Customizing Your Classifier","text":"<ol> <li>Add or Modify Labels: Edit <code>prompt.yaml</code> to add new categories</li> <li>Improve Examples: Add more diverse examples to improve classification</li> <li>Adjust System Message: Customize the initial instructions</li> <li>Switch Models: Try different models with the <code>.with_model()</code> method</li> </ol>"},{"location":"usage/getting-started/#running-evaluations","title":"Running Evaluations","text":"<p>Test your classifier's performance:</p> <pre><code>instruct-classify eval --config configs/example.yaml\n</code></pre> <p>The evaluation generates a detailed report with metrics, visualizations, and insights into model performance.</p>"},{"location":"usage/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about the Evaluation Framework for benchmarking</li> <li>Check the Examples for advanced usage patterns</li> <li>Refer to the API Reference for detailed documentation</li> </ul>"},{"location":"usage/programmatic-definition/","title":"Programmatic Definition","text":"<p>While YAML is convenient for version control and sharing, you can also define your classification schema programmatically in Python code. This page shows examples of creating <code>ClassificationDefinition</code> objects directly, with increasing levels of complexity.</p>"},{"location":"usage/programmatic-definition/#basic-example-direct-object-creation","title":"Basic Example: Direct Object Creation","text":"<p>At its simplest, you can create a classification schema with just labels:</p> <pre><code>from instructor_classify.schema import ClassificationDefinition, LabelDefinition\nfrom instructor_classify.classify import Classifier\nimport instructor\nfrom openai import OpenAI\n\n# Create label definitions\nspam_label = LabelDefinition(\n    label=\"spam\",\n    description=\"Unsolicited messages trying to sell products or scam recipients\"\n)\n\nnot_spam_label = LabelDefinition(\n    label=\"not_spam\",\n    description=\"Legitimate messages with relevant content for the recipient\"\n)\n\n# Create classification definition\nclassification_def = ClassificationDefinition(\n    system_message=\"You are an expert spam detection system.\",\n    label_definitions=[spam_label, not_spam_label]\n)\n\n# Use the classification definition with a classifier\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(classification_def)\n    .with_client(client)\n    .with_model(\"gpt-3.5-turbo\")\n)\n\n# Make a prediction\nresult = classifier.predict(\"CLICK HERE NOW to claim your FREE iPhone 15 Pro Max! You've been selected!\")\nprint(f\"Classification: {result.label}\")  # Should output \"spam\"\n</code></pre>"},{"location":"usage/programmatic-definition/#intermediate-adding-few-shot-examples","title":"Intermediate: Adding Few-Shot Examples","text":"<p>Adding examples helps improve model performance by demonstrating what belongs in each category:</p> <pre><code>from instructor_classify.schema import ClassificationDefinition, LabelDefinition, Examples\nfrom instructor_classify.classify import Classifier\nimport instructor\nfrom openai import OpenAI\n\n# Create label definitions with examples\nspam_label = LabelDefinition(\n    label=\"spam\",\n    description=\"Unsolicited messages trying to sell products or scam recipients\",\n    examples=Examples(\n        examples_positive=[\n            \"CONGRATULATIONS! You've been selected for our exclusive prize! Click here to claim now!\",\n            \"URGENT: Your account has been compromised. Reply with your password to secure your account.\",\n            \"Make $5000 weekly working from home! Limited time offer, sign up now!\"\n        ],\n        examples_negative=[\n            \"Could you send me the report by tomorrow?\",\n            \"Your flight to London has been confirmed for May 15th.\",\n            \"Thanks for your inquiry. Our support team will contact you within 24 hours.\"\n        ]\n    )\n)\n\nnot_spam_label = LabelDefinition(\n    label=\"not_spam\", \n    description=\"Legitimate messages with relevant content for the recipient\",\n    examples=Examples(\n        examples_positive=[\n            \"Your package has been delivered to the front door.\",\n            \"I'm following up on our meeting from yesterday. Are you available tomorrow?\",\n            \"Your monthly statement is now available. Please log in to view.\"\n        ],\n        examples_negative=[\n            \"YOU'VE WON THE LOTTERY! Claim your $1,000,000 prize now!\",\n            \"Hot singles in your area want to meet you tonight!\",\n            \"Your computer has a virus! Call this number immediately!\"\n        ]\n    )\n)\n\n# Create classification definition\nclassification_def = ClassificationDefinition(\n    system_message=\"You are an expert spam detection system. Analyze the given text and determine if it is spam or not.\",\n    label_definitions=[spam_label, not_spam_label]\n)\n\n# Use the classification definition with a classifier\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(classification_def)\n    .with_client(client)\n    .with_model(\"gpt-3.5-turbo\")\n)\n\n# Make predictions\nmessages = [\n    \"URGENT: Your iCloud account has been locked. Click here to verify your identity.\",\n    \"Hi Sarah, here are the documents you requested for the Johnson project.\",\n    \"FREE VACATION ALERT: You've been selected for an all-expenses-paid trip to Hawaii!\",\n    \"Your order #12345 has been shipped and will arrive on Tuesday.\"\n]\n\nfor message in messages:\n    result = classifier.predict(message)\n    print(f\"Message: '{message}'\")\n    print(f\"Classification: {result.label}\")\n    print(\"---\")\n</code></pre>"},{"location":"usage/programmatic-definition/#advanced-building-a-complex-schema-programmatically","title":"Advanced: Building a Complex Schema Programmatically","text":"<p>For more sophisticated classifiers, you might want to generate the schema programmatically:</p> <pre><code>from instructor_classify.schema import ClassificationDefinition, LabelDefinition, Examples\nfrom instructor_classify.classify import Classifier\nimport instructor\nfrom openai import OpenAI\nimport json\n\n# Define spam categories and examples\nspam_categories = {\n    \"phishing\": {\n        \"description\": \"Messages attempting to trick users into revealing sensitive information\",\n        \"examples_positive\": [\n            \"Dear customer, we've detected suspicious activity on your account. Click here to verify your identity.\",\n            \"Your PayPal account has been limited. Login now to restore full access: [suspicious link]\",\n            \"Attention: Your tax refund is pending. Please confirm your banking details within 24 hours.\"\n        ],\n        \"examples_negative\": [\n            \"Please reset your password by clicking the official link in this email from our domain.\",\n            \"We've noticed unusual activity and have temporarily frozen your account. Please call our official number.\"\n        ]\n    },\n    \"promotional\": {\n        \"description\": \"Unsolicited marketing messages promoting products or services\",\n        \"examples_positive\": [\n            \"LIMITED TIME OFFER: 80% OFF ALL PRODUCTS! SHOP NOW!\",\n            \"You've been selected for our EXCLUSIVE VIP discount! Click now before it expires!\",\n            \"Buy one get THREE free! This week only at SuperStore!\"\n        ],\n        \"examples_negative\": [\n            \"As a subscriber to our newsletter, here's your monthly discount code.\",\n            \"Thank you for your purchase. Here are other products you might enjoy.\"\n        ]\n    },\n    \"scam\": {\n        \"description\": \"Fraudulent messages intended to deceive recipients for financial gain\",\n        \"examples_positive\": [\n            \"I am Prince Abioye from Nigeria. I need your help to transfer $5,000,000 USD.\",\n            \"CONGRATULATIONS! You've WON the Microsoft lottery! Send $100 processing fee to claim $1,000,000!\",\n            \"Your computer has been infected with a virus! Call this number immediately to remove it.\"\n        ],\n        \"examples_negative\": [\n            \"You've won our legitimate contest. No purchase necessary to claim your prize.\",\n            \"We've detected suspicious activity on your account. Please call the number on the back of your card.\"\n        ]\n    }\n}\n\n# Define legitimate categories\nnot_spam_categories = {\n    \"personal\": {\n        \"description\": \"Genuine personal communications between individuals\",\n        \"examples_positive\": [\n            \"Hey, are we still meeting for lunch tomorrow at 12?\",\n            \"I've attached the photos from our trip. Hope you like them!\",\n            \"Just checking in to see how you're doing. Call me when you get a chance.\"\n        ]\n    },\n    \"business\": {\n        \"description\": \"Legitimate business communications\",\n        \"examples_positive\": [\n            \"Your invoice #1234 is attached. Payment is due within 30 days.\",\n            \"The meeting has been rescheduled to Tuesday at 2pm in Conference Room A.\",\n            \"Thank you for your application. We'd like to invite you for an interview.\"\n        ]\n    },\n    \"transactional\": {\n        \"description\": \"Legitimate automated notifications related to user actions or accounts\",\n        \"examples_positive\": [\n            \"Your payment of $50.00 was processed successfully.\",\n            \"Your order has shipped. Tracking number: TRK123456789\",\n            \"Your password was changed at 3:45 PM. If this wasn't you, please contact support.\"\n        ]\n    }\n}\n\n# Build label definitions programmatically\nlabel_definitions = []\n\n# Add spam categories with subcategories\nfor category, details in spam_categories.items():\n    label = f\"spam_{category}\"\n    examples = Examples(\n        examples_positive=details[\"examples_positive\"],\n        examples_negative=details.get(\"examples_negative\", [])\n    )\n    label_def = LabelDefinition(\n        label=label,\n        description=details[\"description\"],\n        examples=examples\n    )\n    label_definitions.append(label_def)\n\n# Add legitimate categories\nfor category, details in not_spam_categories.items():\n    label = f\"legitimate_{category}\"\n    examples = Examples(\n        examples_positive=details[\"examples_positive\"],\n        examples_negative=details.get(\"examples_negative\", [])\n    )\n    label_def = LabelDefinition(\n        label=label,\n        description=details[\"description\"],\n        examples=examples\n    )\n    label_definitions.append(label_def)\n\n# Create the classification definition\nsystem_message = \"\"\"\nYou are an advanced spam detection system with the ability to categorize different types of messages.\nAnalyze the given text and determine both whether it is spam and what specific type of message it is.\n\"\"\"\n\nclassification_def = ClassificationDefinition(\n    system_message=system_message,\n    label_definitions=label_definitions\n)\n\n# Use the classifier\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(classification_def)\n    .with_client(client)\n    .with_model(\"gpt-4o-mini\")  # Using a more capable model for fine-grained classification\n)\n\n# Test messages\ntest_messages = [\n    \"Dear valued customer, we have detected suspicious activity on your bank account. Please verify your identity by clicking this link: http://suspiciouslink.com\",\n    \"Hey John, just wanted to confirm our meeting tomorrow at 2pm. Let me know if that still works for you. Cheers, Maria\",\n    \"CONGRATULATIONS! You've been selected to receive a FREE iPhone 15! Click here to claim your prize now! Limited time offer!\",\n    \"Your Amazon order #AB12345 has been shipped and will arrive on Thursday, April 20. Track your package here: [legitimate tracking link]\",\n    \"I am a wealthy businessman who needs your help. I can transfer $5,000,000 to your account if you pay a small fee of $1,000 first.\"\n]\n\nfor message in test_messages:\n    result = classifier.predict(message)\n    print(f\"Message: '{message[:50]}...'\")\n    print(f\"Classification: {result.label}\")\n    print(\"---\")\n\n# Optional: Save the definition to YAML for version control\nimport yaml\n\ndef export_to_yaml(classification_def, file_path):\n    \"\"\"Convert a ClassificationDefinition to YAML and save it to a file.\"\"\"\n    # Convert the Pydantic model to a dictionary\n    data = classification_def.model_dump()\n\n    # Save to YAML\n    with open(file_path, 'w') as file:\n        yaml.dump(data, file, default_flow_style=False)\n\n    print(f\"Saved classification definition to {file_path}\")\n\n# Export the programmatically created definition\nexport_to_yaml(classification_def, \"advanced_spam_classifier.yaml\")\n</code></pre>"},{"location":"usage/programmatic-definition/#hybrid-approach-loading-from-dict-or-json","title":"Hybrid Approach: Loading from Dict or JSON","text":"<p>You can also create a classification definition from a dictionary or JSON:</p> <pre><code>from instructor_classify.schema import ClassificationDefinition\nimport json\n\n# Define your schema as a Python dictionary\nschema_dict = {\n    \"system_message\": \"You are an expert spam detection system.\",\n    \"label_definitions\": [\n        {\n            \"label\": \"spam\",\n            \"description\": \"Unsolicited messages trying to sell products or scam recipients\",\n            \"examples\": {\n                \"examples_positive\": [\n                    \"URGENT: You have won $1,000,000 in the lottery!\",\n                    \"Click here for a FREE iPhone!\"\n                ],\n                \"examples_negative\": [\n                    \"Your meeting is scheduled for tomorrow at 2pm.\",\n                    \"Thanks for your purchase, here's your receipt.\"\n                ]\n            }\n        },\n        {\n            \"label\": \"not_spam\",\n            \"description\": \"Legitimate messages with relevant content for the recipient\",\n            \"examples\": {\n                \"examples_positive\": [\n                    \"Your Amazon order has shipped.\",\n                    \"Meeting notes from yesterday's call.\"\n                ],\n                \"examples_negative\": [\n                    \"MAKE MONEY FAST! Work from home!\",\n                    \"Your account has been compromised! Click here!\"\n                ]\n            }\n        }\n    ]\n}\n\n# Create from dictionary\nclassification_def = ClassificationDefinition(**schema_dict)\n\n# Alternative: Load from JSON file\ndef load_from_json(file_path):\n    with open(file_path, 'r') as file:\n        data = json.load(file)\n    return ClassificationDefinition(**data)\n\n# If you have a JSON file\n# classification_def = load_from_json(\"spam_classifier.json\")\n\n# Use with classifier as before\nfrom instructor_classify.classify import Classifier\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\nclassifier = (\n    Classifier(classification_def)\n    .with_client(client)\n    .with_model(\"gpt-3.5-turbo\")\n)\n\n# Test\nresult = classifier.predict(\"CONGRATULATIONS! You've won a free cruise! Call now to claim!\")\nprint(f\"Classification: {result.label}\")\n</code></pre>"},{"location":"usage/programmatic-definition/#benefits-of-programmatic-definition","title":"Benefits of Programmatic Definition","text":"<p>Programmatic definition offers several advantages:</p> <ol> <li>Dynamic Creation: Generate schemas based on user input or database content</li> <li>Validation: Perform additional validation or transformation before creating the schema</li> <li>Integration: Pull examples from existing datasets or databases</li> <li>Flexibility: Modify the schema at runtime based on performance or feedback</li> <li>Code Control: Keep everything in Python rather than separate YAML files</li> <li>Testing: Easier to create test fixtures and variations</li> </ol> <p>However, for production use cases, it's still recommended to export your programmatically created definitions to YAML files for version control and easy sharing with team members.</p>"}]}